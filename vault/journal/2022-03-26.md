# 2022-03-26
 yesterday i managed to cough up a rough draft of the new tab screen 
 today, i want to finish it, and try it out.
 


 add "check for updates" in quick file menu
 


 Why did i have todo a heroic act for everything to keep working?
    ## Question the very premiss

What can / should / could be done to remove the duct tape?
    



Love misery.
    
    monitoring is broken
    so much that there is no fixing
    appreciate misery


    Gil Tene
        Azul Systems
        Garbage Collection
        wrk2

    The sensisitivity of the whole system as a whole increases when moving to microservices architectures 
        . 
        . Loeads to evolving tools
            .. Circuity breaks
            .. Backpressure mechanisms 
            .. Hystrix, istio, conduit, linkerd , consul

        . Prevent meltdown by flipping before hitting 
        . Create Monitoring systems , etc etc etc
            .. High cardinality, can't see it in dashboards, try to simplify it into flow etc.

    
    We like to look at pretty charts. 
        "We only want to show good things"
            95% chart : we discarded the worst things. Seeing a spikle means that 5% of things had to be terrible , worst than the spike.
        
        the average is non-sense because latency its not gaussian distributed.

        Since a user interacts in a compound way ( pages and related requests ), there are actually very high chances that it will hit at least one of the percentiles:
         "
         If a typicial user sessionm oinvolves 5 page loads, averaging 40 resources per page. 
         How mnay of our users will not experience something worse than the 95% of http requests?
            0.003%.
        "
        So why is the 95% relevant?
            it isn't. It  makes more sense to go backwards and find out what we need to watch for.

        "
        If a typiical user session involves 5 pager loads, averaging 40 resources per page

        what http response percentile will be experienced by the 
            95% percentile of users?99.97
            99% percentile? 99.995

        the median is also irrelevant

        wishful thinking
            if i cover 99%, how bad can the rest of the stuff be?

        mean and standard deviations are only useful in case we have a gaussian distributinos 
            
            t. in the real world, latency distributions are heavily, heavily skewed 
                .. common to have your 99.999 be 1000x greater than your 99
            
            check HdrHistogram
            
            . coordinated omission problem
                .. accidental conspiracy
                example:
                    
                    start = time()
                        do
                    finish = time()
                        return finish - start
                the trouble is that long operations onlu get measured once
                delay outside of timing window do not get measured at all
                
                "vertical rises in the latency tend to indicate omission"

                very few tools deal with it correctly.


                Service time vs resonse time
                    . Think of a queue
                    . nobody experiences service time 
                    . the gap between the two grows with load.
                    . if we go past the 'limit' ( saturation ), the service time stay the same, and response time goes through the roof.
                    . Response time grows linearly past the saturation limit. 

                    . latency does not lives in isolation, we need to look ait it in context of load
                        .. looking at how systems behave when pushed past the saturation point is useless: you already crashed your car.
                        .. very close to saturation is also useless
                    
                focus on measuring misery 
                    "Don't aim to make a % acceptable, 
                    instead, aim for an acceptable % of misery"

    https://www.youtube.com/watch?v=lJ8ydIuPFeU
    https://www.youtube.com/watch?v=nS0QgxgUYSA

#


Stop Rate Limiting Capacity Management Done ERight
    Queueing theory
    

    API Management Gateway
        authentication and authorizatoin

        capacity management 
            don't let one client starve the others one

            often, they rate limit.
                . when the origin service has overloaded
                . sometimes rate limit is too aggressive: we could have served the request
            
            N = XR 
            ( LIttles law )
            Average # of concurrent requests = Average Transaction Rate * 
            Average response time
            
            
            "Under normal operating conditions, limiting concurrent requests does the same as rate limiting"
                .. This introduces backpressure
            

            throughput != capacity 

            TCP congestion avoidance control 
                . Additive Increase
                . Multiplicative decrease
            

https://www.youtube.com/watch?v=m64SWl9bfvk



#


html contents can be express in og ( open graph ) or in rdf tags too. Also in script of type text/front-matter
Also, look through headers. 



#
    Control Theory


    Sketch a root locus

    Proportional COntroll is the simplest control
        .Not necessarily solves all porbolems
        .sometimes you can move root to the left half plane or have a negative real part
        . interesting to understand how poles and zeros affect the root locus. 
         
         in transfer functions, the numerator controls the system behave different due to force inputs. 
    
        when nPoles <> nZeroes :
            if nPoles > nZeroes, the extra poles goes to infinity
            if nPoles < nZeroes, the extra zeros comes from infinity


https://www.youtube.com/watch?v=eTVddYCeiKI
        


# 
https://towardsdatascience.com/glcms-a-great-tool-for-your-ml-arsenal-7a59f1e45b65

Gray level co-occurence matrices

    extract texture features from images
        
    isnt this just convolutions? and the positional operator is your kernel?
    
    whoever, it raises some good points:
        . We can calculate the entropy of a matrix 
            -sum( c[i,j] * log( c[i,j]) )
        . We can calculate the correlation of a matrix
            -sum( 
                ( i-u[i] * ( j - u[j]) * c[i,j]) /
                (theta[i] * theta[j]) 
                 )
                where 
                u[k] = sum( k * c[i, j])
                theta^2[k]  = sum( (c[i , j ]  * u[k] )**2)
        . And also , the homogeneity and contrast

    associated with texture == repetition of visual patterns == repetition ofcombinations fo values with a certain orientations 



    separate rgb AND  hsv channels
        hsv channels are closer to how humans interpret vision 
            => Here we can put a link to that image compression video from irreducible


        assymetrical costs of classification
    
    what i found interesting here is the transformation  from matrices to individual numbers:
        
        R-Matrix
        G-Matrix
        B-Matrix
        
        GrayScaleMatrix ( By Averaging RGB )

        H-Matrix
        S-Matrix
        V-Matrix
        
        \/ Calculate a GLCM of each , and for each glcm, compute its 4 metrics ( homogoneity , contrast, energy , correlation )
            > Which are analogous to mean, std, entropy and correlation . 
        
        7 * 4 = 28 numerical features



# 

https://towardsdatascience.com/drf-a-random-forest-for-almost-everything-625fa5c3bcb8

https://github.com/lorismichel/drf

Distributional Random Forests


MMD, Kernels 

"Though the magic of kernel methods, this comparison  between mesn is actuly a comparion of distributions"

MMD allows to compare multivariate distributions
    ( Maximum Mean Discrepance ? )


Can we Make any paralells with th NaturalGradientForets?

https://github.com/stanfordmlgroup/ngboost



#

        https://www.youtube.com/shorts/Fbmru6iSee8

https://www.youtube.com/watch?v=6cncmSaRqzQ

HTTP
    Request - Response Protocol
    Does not store anything anywhere == stateless
        This is where the Http 1.0 makes a mistake:
            Design so that you can statellessly send a request, but requires a re-stablishing the TCP protocol everytime 
                . To be truly stateless.
                . Save memory ( Long running TCP connectoins use resources )
                    .. Only long running?



#

https://www.youtube.com/watch?v=6cncmSaRqzQ

Turning HTTP 2 was a mistake

    Binary compression for headers
    decreases latency by multiplexing requests
    allows client to specify priorities for requests 
    

    In this case, they switched the protocol only at the client side of the load balancer , while the load balancer to cdn and cdn to backend was stilll on http 1
    
    usually, when on http1, the broswer opens 6-10 connections , which stablishes a upper limit of parallelism for request. Any further requests must tbe queueed at the client side. 

    When the load balancer switched to http 2 ( at the front side ) , a sudden load appeared at the backend 
        This means an single tcp connection can handle around 100 requests

        Since the load balancer receives 100 requests, 
            and the backend works with http1 , which requires a tcp connection for every request, 
                the load balancer opens 100 tcp connections 
    
    logically speaking, sure , http2 will consume more resources ( cpu )
        At the Application  level , it is reading packets and waiting for them to be assembled into streams 
            -> Sorting and working with these streams can consume more resources
            -> Google http 2 cpu and http 1 slow start? 
            

#

https://www.youtube.com/watch?v=edB-_JnhoRY

Content Persistence




                
    